{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载人民日报数据集\n",
    "! wget http://s3.bmio.net/kashgari/china-people-daily-ner-corpus.tar.gz\n",
    "# 解压\n",
    "! unzip china-people-daily-ner-corpus.tar.gz\n",
    "# 下载bert\n",
    "! wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip  \n",
    "# 解压\n",
    "! unzip chinese_L-12_H-768_A-12.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip下载bert4keras包\n",
    "!pip install bert4keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代码参考：\n",
    "import re, os, json\n",
    "import numpy as np\n",
    "from bert4keras.backend import keras, K\n",
    "from bert4keras.bert import build_bert_model\n",
    "from bert4keras.tokenizer import Tokenizer\n",
    "from bert4keras.optimizers import Adam\n",
    "from bert4keras.snippets import sequence_padding, DataGenerator\n",
    "from bert4keras.snippets import open\n",
    "from bert4keras.layers import ConditionalRandomField\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 标注数据\n",
    "def load_data(filename):\n",
    "    D = []\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        f = f.read()\n",
    "        # f = f.replace('  ','。 ') # 存疑\n",
    "        f = f.replace('  O','可 O') # 存疑\n",
    "        f = f.replace('E-','I-') # 存疑 #因为没有E-只有I-\n",
    "        for l in f.split('\\n\\n'):\n",
    "            if not l:\n",
    "                continue\n",
    "            d, last_flag = [], ''\n",
    "            for c in l.split('\\n'):\n",
    "                char, this_flag = c.split(' ')\n",
    "                if this_flag == 'O' and last_flag == 'O':\n",
    "                    d[-1][0] += char\n",
    "                elif this_flag == 'O' and last_flag != 'O':\n",
    "                    d.append([char, 'O'])\n",
    "                elif this_flag[:1] == 'B':\n",
    "                    d.append([char, this_flag[2:]])\n",
    "                else:\n",
    "                    d[-1][0] += char\n",
    "                last_flag = this_flag\n",
    "            D.append(d)\n",
    "    return D\n",
    "    \n",
    "# 载入数据，注意已经把下载好的人民日报数据放到了./sample_data文件夹内\n",
    "train_data = load_data('./sample_data/example.train')\n",
    "valid_data = load_data('./sample_data/example.dev')\n",
    "test_data = load_data('./sample_data/example.test')\n",
    "\n",
    "# bert配置\n",
    "config_path = './chinese_L-12_H-768_A-12/bert_config.json'\n",
    "checkpoint_path = './chinese_L-12_H-768_A-12/bert_model.ckpt'\n",
    "dict_path = './chinese_L-12_H-768_A-12/vocab.txt'\n",
    "\n",
    "# 建立分词器\n",
    "tokenizer = Tokenizer(dict_path, do_lower_case=True)\n",
    "\n",
    "# 类别映射，数据集改变时可能需要注意调整，否则会在下一个代码块报错提示出现未出现的标签\n",
    "classes = set(['PER', 'LOC', 'ORG'])\n",
    "\n",
    "id2class = dict(enumerate(classes))\n",
    "class2id = {j: i for i, j in id2class.items()}\n",
    "num_labels = len(classes) * 2 + 1\n",
    "\n",
    "maxlen = 256\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "bert_layers = 12\n",
    "learing_rate = 1e-5  # bert_layers越小，学习率应该要越大\n",
    "crf_lr_multiplier = 1000  # 必要时扩大CRF层的学习率\n",
    "\n",
    "class data_generator(DataGenerator):\n",
    "    \"\"\"数据生成器\n",
    "    \"\"\"\n",
    "    def __iter__(self, random=False):\n",
    "        idxs = list(range(len(self.data)))\n",
    "        if random:\n",
    "            np.random.shuffle(idxs)\n",
    "        batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
    "        for i in idxs:\n",
    "            token_ids, labels = [tokenizer._token_cls_id], [0]\n",
    "            for w, l in self.data[i]:\n",
    "                w_token_ids = tokenizer.encode(w)[0][1:-1]\n",
    "                if len(token_ids) + len(w_token_ids) < maxlen:\n",
    "                    token_ids += w_token_ids\n",
    "                    if l == 'O':\n",
    "                        labels += [0] * len(w_token_ids)\n",
    "                    else:\n",
    "                        B = class2id[l] * 2 + 1\n",
    "                        I = class2id[l] * 2 + 2\n",
    "                        labels += ([B] + [I] * (len(w_token_ids) - 1))\n",
    "                else:\n",
    "                    break\n",
    "            token_ids += [tokenizer._token_sep_id]\n",
    "            labels += [0]\n",
    "            segment_ids = [0] * len(token_ids)\n",
    "            batch_token_ids.append(token_ids)\n",
    "            batch_segment_ids.append(segment_ids)\n",
    "            batch_labels.append(labels)\n",
    "            if len(batch_token_ids) == self.batch_size or i == idxs[-1]:\n",
    "                batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                batch_segment_ids = sequence_padding(batch_segment_ids)\n",
    "                batch_labels = sequence_padding(batch_labels)\n",
    "                yield [batch_token_ids, batch_segment_ids], batch_labels\n",
    "                batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
    "\n",
    "model = build_bert_model(\n",
    "    config_path,\n",
    "    checkpoint_path,\n",
    ")\n",
    "\n",
    "output_layer = 'Encoder-%s-FeedForward-Norm' % bert_layers\n",
    "output = model.get_layer(output_layer).output\n",
    "output = Dense(num_labels)(output)\n",
    "CRF = ConditionalRandomField(lr_multiplier=crf_lr_multiplier)\n",
    "output = CRF(output, mask='Sequence-Mask')\n",
    "\n",
    "model = Model(model.input, output)\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=CRF.sparse_loss,\n",
    "              optimizer=Adam(learing_rate),\n",
    "              metrics=[CRF.sparse_accuracy])\n",
    "def viterbi_decode(nodes, trans):\n",
    "    \"\"\"Viterbi算法求最优路径\n",
    "    其中nodes.shape=[seq_len, num_labels],\n",
    "        trans.shape=[num_labels, num_labels].\n",
    "    \"\"\"\n",
    "    labels = np.arange(num_labels).reshape((1, -1))\n",
    "    scores = nodes[0].reshape((-1, 1))\n",
    "    scores[1:] -= np.inf  # 第一个标签必然是0\n",
    "    paths = labels\n",
    "    for l in range(1, len(nodes)):\n",
    "        M = scores + trans + nodes[l].reshape((1, -1))\n",
    "        idxs = M.argmax(0)\n",
    "        scores = M.max(0).reshape((-1, 1))\n",
    "        paths = np.concatenate([paths[:, idxs], labels], 0)\n",
    "    return paths[:, scores[0].argmax()]\n",
    "\n",
    "\n",
    "def named_entity_recognize(text):\n",
    "    \"\"\"命名实体识别函数\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    while len(tokens) > 512:\n",
    "        tokens.pop(-2)\n",
    "    token_ids = tokenizer.tokens_to_ids(tokens)\n",
    "    segment_ids = [0] * len(token_ids)\n",
    "    nodes = model.predict([[token_ids], [segment_ids]])[0]\n",
    "    trans = K.eval(CRF.trans)\n",
    "    labels = viterbi_decode(nodes, trans)[1:-1]\n",
    "    entities, starting = [], False\n",
    "    for token, label in zip(tokens[1:-1], labels):\n",
    "        if label > 0:\n",
    "            if label % 2 == 1:\n",
    "                starting = True\n",
    "                entities.append([[token], id2class[(label - 1) // 2]])\n",
    "            elif starting:\n",
    "                entities[-1][0].append(token)\n",
    "            else:\n",
    "                starting = False\n",
    "        else:\n",
    "            starting = False\n",
    "    return [(tokenizer.decode(w, w).replace(' ', ''), l) for w, l in entities]\n",
    "\n",
    "\n",
    "def evaluate(data):\n",
    "    \"\"\"评测函数\n",
    "    \"\"\"\n",
    "    X, Y, Z = 1e-10, 1e-10, 1e-10\n",
    "    for d in tqdm(data):\n",
    "        text = ''.join([i[0] for i in d])\n",
    "        R = set(named_entity_recognize(text))\n",
    "        T = set([tuple(i) for i in d if i[1] != 'O'])\n",
    "        X += len(R & T)\n",
    "        Y += len(R)\n",
    "        Z += len(T)\n",
    "    f1, precision, recall = 2 * X / (Y + Z), X / Y, X / Z\n",
    "    return f1, precision, recall\n",
    "\n",
    "\n",
    "class Evaluate(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.best_val_f1 = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        trans = K.eval(CRF.trans)\n",
    "        print(trans)\n",
    "        f1, precision, recall = evaluate(valid_data)\n",
    "        # 保存最优\n",
    "        if f1 >= self.best_val_f1:\n",
    "            self.best_val_f1 = f1\n",
    "            model.save_weights('./best_model.weights')\n",
    "        print('valid:  f1: %.5f, precision: %.5f, recall: %.5f, best f1: %.5f\\n' %\n",
    "              (f1, precision, recall, self.best_val_f1))\n",
    "        f1, precision, recall = evaluate(test_data)\n",
    "        print('test:  f1: %.5f, precision: %.5f, recall: %.5f\\n' %\n",
    "              (f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluate()\n",
    "batch_size = 10\n",
    "epochs = 10\n",
    "train_generator = data_generator(train_data, batch_size)\n",
    "# 进行训练\n",
    "history = model.fit_generator(train_generator.forfit(),\n",
    "                steps_per_epoch=len(train_generator),\n",
    "                epochs=epochs,\n",
    "                callbacks=[evaluator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测\n",
    "text = 'MALLET是美国麻省大学（UMASS）阿姆斯特（Amherst）分校开发的一个统计自然语言处理开源软件包'\n",
    "named_entity_recognize(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
